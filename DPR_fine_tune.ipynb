{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c4bb4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:20:19.451265915Z",
     "start_time": "2023-08-25T17:20:06.795485554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "training:   7%|▋         | 16/233 [00:02<00:37,  5.79it/s]\n",
      "  0%|          | 0/3 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 105\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    108\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/data/installation/anaconda3/envs/negative/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/installation/anaconda3/envs/negative/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/data/installation/anaconda3/envs/negative/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/installation/anaconda3/envs/negative/lib/python3.8/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/data/installation/anaconda3/envs/negative/lib/python3.8/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/installation/anaconda3/envs/negative/lib/python3.8/site-packages/torch/optim/adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    318\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "# Prepare Data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, questions, contexts):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.questions[idx], self.contexts[idx]\n",
    "    \n",
    "    \n",
    "def dpr_criterion(scores):\n",
    "    return -torch.mean(torch.log(torch.diag(F.softmax(scores, dim=1))))\n",
    "    \n",
    "device = torch.device(\"cuda:0\") \n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# df = pd.read_csv('../postprocess/all_examples_0601_hate.csv', sep = \"\\t\").reset_index(drop=True)\n",
    "df = pd.read_csv(\"all_examples_sorted.csv\", sep = \"\\t\").reset_index(drop=True)\n",
    "#contexts = df.loc[df['guideline'] == 'filth', 'sentence']\n",
    "contexts = df['sentence']\n",
    "questions = df['guideline']\n",
    "\n",
    "\n",
    "dataset = MyDataset(questions, contexts)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "print(len(dataset))\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "batch_size = 16\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset = Subset(dataset, indices=range(0, train_size))\n",
    "val_dataset = Subset(dataset, indices=range(train_size, train_size + val_size))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize DPR model and tokenizer\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW([\n",
    "    {'params': context_model.parameters()},\n",
    "    {'params': question_model.parameters()}\n",
    "], lr=1e-5)\n",
    "\n",
    "epochs = 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "# Loss function\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in trange(epochs):\n",
    "    context_model.train()\n",
    "    question_model.train()\n",
    "    \n",
    "    # Training\n",
    "    with tqdm(total=len(train_dataloader), desc=\"training\", miniters=20) as pbar:\n",
    "        for batch in train_dataloader:\n",
    "            questions, passages = batch\n",
    "\n",
    "            question_inputs = question_tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            passage_inputs = context_tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            question_outputs = question_model(**question_inputs).pooler_output\n",
    "            passage_outputs = context_model(**passage_inputs).pooler_output\n",
    "            \n",
    "#             print(question_outputs.shape)\n",
    "#             print(passage_outputs.shape)\n",
    "\n",
    "            # targets = torch.diag(torch.ones(len(questions))).to(device)\n",
    "            similarity_scores = torch.matmul(question_outputs, torch.transpose(passage_outputs, 0, 1))\n",
    "            loss = dpr_criterion(similarity_scores)\n",
    "\n",
    "            \n",
    "#             print(targets)\n",
    "#             print(similarity_scores)\n",
    "            \n",
    "            # loss = criterion(similarity_scores, targets)\n",
    "#            print(loss)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Validation\n",
    "    context_model.eval()\n",
    "    question_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            questions, passages = batch\n",
    "            question_inputs = question_tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            passage_inputs = context_tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            question_outputs = question_model(**question_inputs).pooler_output\n",
    "            passage_outputs = context_model(**passage_inputs).pooler_output \n",
    "            \n",
    "            # targets = torch.diag(torch.ones(len(questions))).to(device)\n",
    "            \n",
    "            \n",
    "            similarity_scores = torch.matmul(question_outputs, torch.transpose(passage_outputs, 0, 1))\n",
    "            loss = dpr_criterion(similarity_scores)\n",
    "#             loss = criterion(similarity_scores, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Validation Loss: {val_loss / len(val_dataloader)}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(context_model.state_dict(), \"context_model.pth\")\n",
    "torch.save(question_model.state_dict(), \"question_model.pth\")\n",
    "\n",
    "# To load model\n",
    "# context_model.load_state_dict(torch.load(\"context_model.pth\"))\n",
    "# question_model.load_state_dict(torch.load(\"question_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4994855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "context_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "question_model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "\n",
    "context_model.load_state_dict(torch.load(\"context_model.pth\"))\n",
    "question_model.load_state_dict(torch.load(\"question_model.pth\"))\n",
    "\n",
    "# def retrieve_passages(question_model, context_model, question_tokenizer, context_tokenizer, query, contexts):\n",
    "#     # Generate question embeddings\n",
    "#     question_inputs = question_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         question_embedding = question_model(**question_inputs).pooler_output\n",
    "\n",
    "#     # Generate context embeddings\n",
    "#     context_inputs = context_tokenizer(contexts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         context_embeddings = context_model(**context_inputs).pooler_output\n",
    "\n",
    "#     # Calculate similarity between question and context embeddings\n",
    "#     similarity_scores = torch.matmul(question_embedding, torch.transpose(context_embeddings, 0, 1)).squeeze()\n",
    "\n",
    "#     # Sort the similarity scores and get the indices of the top 5 similar contexts\n",
    "#     sorted_indices = torch.argsort(similarity_scores, descending=True)[:5]\n",
    "\n",
    "#     # Retrieve the top 5 similar context passages\n",
    "#     top_contexts = [contexts[idx] for idx in sorted_indices.tolist()]\n",
    "\n",
    "#     return top_contexts\n",
    "\n",
    "# def compute_relevance(query, context):\n",
    "#     # Generate question embeddings\n",
    "#     question_inputs = question_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#     context_inputs = context_tokenizer(contexts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         question_embedding = question_model(**question_inputs).pooler_output\n",
    "#         context_embeddings = context_model(**context_inputs).pooler_output\n",
    "\n",
    "#         # Calculate similarity between question and context embeddings\n",
    "#         similarity_score = torch.matmul(question_embedding, torch.transpose(context_embeddings, 0, 1)).squeeze()\n",
    "\n",
    "#     return similarity_score.cpu().numpy()\n",
    "\n",
    "# df = pd.read_csv('../postprocess/all_examples_0601_hate.csv', sep = \"\\t\").reset_index(drop=True)\n",
    "# #contexts = df.loc[df['guideline'] == 'filth', 'sentence']\n",
    "\n",
    "# # Your list of context passages\n",
    "# query = \"filth\"\n",
    "# contexts = df['sentence'][:].tolist()\n",
    "\n",
    "# for context in tqdm(contexts):\n",
    "#     score = compute_relevance(query, context)\n",
    "#     scores.append(score)\n",
    "\n",
    "\n",
    "\n",
    "# # Perform retrieval\n",
    "# top_contexts = retrieve_passages(\n",
    "#     question_model,\n",
    "#     context_model,\n",
    "#     question_tokenizer,\n",
    "#     context_tokenizer,\n",
    "#     query,\n",
    "#     contexts\n",
    "# )\n",
    "\n",
    "# # Display the results\n",
    "# for i, context in enumerate(top_contexts):\n",
    "#     print(f\"Rank {i+1}: {context}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c03bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4651/4651 [01:08<00:00, 67.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_top_documents(query, df, topk=5):\n",
    "    scores = list()\n",
    "    for context in tqdm(contexts):\n",
    "        score = compute_relevance(query, context)\n",
    "        scores.append(score)\n",
    "    \n",
    "    df[\"score\"] = scores\n",
    "    return df.sort_values(by=\"score\", ascending=False).sentence.tolist()[:topk]\n",
    "    \n",
    "\n",
    "def compute_relevance(query, context):    \n",
    "    # Generate question embeddings\n",
    "    question_inputs = question_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    context_inputs = context_tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        question_embedding = question_model(**question_inputs).pooler_output\n",
    "        context_embeddings = context_model(**context_inputs).pooler_output\n",
    "\n",
    "        # Calculate similarity between question and context embeddings\n",
    "        similarity_score = torch.matmul(question_embedding, torch.transpose(context_embeddings, 0, 1)).squeeze()\n",
    "\n",
    "    return similarity_score.cpu().numpy()\n",
    "    \n",
    "\n",
    "query = \"Mocking others due to their personality.\"\n",
    "df = pd.read_csv('../postprocess/all_examples_0601_hate.csv', sep = \"\\t\").reset_index(drop=True)\n",
    "\n",
    "retrieved_documents = get_top_documents(query, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc8cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Your lives aren't worth shit to me. You're just Muslims.\",\n",
      " \"the real virus isn't corona; it's the gays\",\n",
      " 'Jews are not welcome here!',\n",
      " 'Indeed. Thieving lying parasites',\n",
      " 'In my opinion, women are worthless dirt.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(retrieved_documents, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae434b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4651/4651 [01:08<00:00, 67.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All Muslim men are potential rapists.',\n",
      " ' dont want to give any rights to queans. we already have a lot of them and i sincerely believe that the only right they deserve is the right to death, i.e. assisted suicide',\n",
      " 'dalits are lowlives',\n",
      " 'trans are crap',\n",
      " 'I am going to make those Muslims suffer.']\n"
     ]
    }
   ],
   "source": [
    "query = \"filth\"\n",
    "retrieved_documents = get_top_documents(query, df)\n",
    "pprint(retrieved_documents, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40fbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"scores\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886fc2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>dataset</th>\n",
       "      <th>index</th>\n",
       "      <th>guideline</th>\n",
       "      <th>votes</th>\n",
       "      <th>labelers</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>11708</td>\n",
       "      <td>I endorse the attacking of gay people</td>\n",
       "      <td>dynahate</td>\n",
       "      <td>1191</td>\n",
       "      <td>attack concept associated protected characteri...</td>\n",
       "      <td>1;1</td>\n",
       "      <td>2;4</td>\n",
       "      <td>46.91195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>11823</td>\n",
       "      <td>I endorse the attacking of homosexuals</td>\n",
       "      <td>dynahate</td>\n",
       "      <td>39198</td>\n",
       "      <td>attack concept associated protected characteri...</td>\n",
       "      <td>1;1</td>\n",
       "      <td>2;4</td>\n",
       "      <td>46.975536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4476</th>\n",
       "      <td>11830</td>\n",
       "      <td>I endorse the attacking of jews</td>\n",
       "      <td>dynahate</td>\n",
       "      <td>39190</td>\n",
       "      <td>attack concept associated protected characteri...</td>\n",
       "      <td>1;1</td>\n",
       "      <td>2;4</td>\n",
       "      <td>47.009098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>11846</td>\n",
       "      <td>I endorse the attacking of muslims</td>\n",
       "      <td>dynahate</td>\n",
       "      <td>39173</td>\n",
       "      <td>attack concept associated protected characteri...</td>\n",
       "      <td>1;1</td>\n",
       "      <td>2;4</td>\n",
       "      <td>47.10845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>11829</td>\n",
       "      <td>I endorse the attacking of jewish people</td>\n",
       "      <td>dynahate</td>\n",
       "      <td>39192</td>\n",
       "      <td>attack concept associated protected characteri...</td>\n",
       "      <td>1;1</td>\n",
       "      <td>2;4</td>\n",
       "      <td>47.176685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>73</td>\n",
       "      <td>Disabled people are worthless dirt.</td>\n",
       "      <td>hatecheck</td>\n",
       "      <td>291</td>\n",
       "      <td>filth</td>\n",
       "      <td>1;1;1</td>\n",
       "      <td>10;11;2</td>\n",
       "      <td>66.559425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>106</td>\n",
       "      <td>You are absolute filth</td>\n",
       "      <td>hatecheck</td>\n",
       "      <td>2996</td>\n",
       "      <td>filth</td>\n",
       "      <td>1;1;1</td>\n",
       "      <td>10;11;2</td>\n",
       "      <td>66.83118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>82</td>\n",
       "      <td>Immigrants like you are absolute filth.</td>\n",
       "      <td>hatecheck</td>\n",
       "      <td>300</td>\n",
       "      <td>filth</td>\n",
       "      <td>1;1;1</td>\n",
       "      <td>10;11;2</td>\n",
       "      <td>67.11209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>103</td>\n",
       "      <td>Immigrants like you are absolute filth.</td>\n",
       "      <td>hatecheck</td>\n",
       "      <td>300</td>\n",
       "      <td>filth</td>\n",
       "      <td>1;1;1</td>\n",
       "      <td>10;11;2</td>\n",
       "      <td>67.11209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>wommennnnnn are dirt</td>\n",
       "      <td>dynahate</td>\n",
       "      <td>4838</td>\n",
       "      <td>filth</td>\n",
       "      <td>1;1;1</td>\n",
       "      <td>10;11;2</td>\n",
       "      <td>67.18407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4651 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      example_id                                  sentence    dataset  index  \\\n",
       "4369       11708     I endorse the attacking of gay people   dynahate   1191   \n",
       "4469       11823    I endorse the attacking of homosexuals   dynahate  39198   \n",
       "4476       11830           I endorse the attacking of jews   dynahate  39190   \n",
       "4492       11846        I endorse the attacking of muslims   dynahate  39173   \n",
       "4475       11829  I endorse the attacking of jewish people   dynahate  39192   \n",
       "...          ...                                       ...        ...    ...   \n",
       "12            73       Disabled people are worthless dirt.  hatecheck    291   \n",
       "36           106                    You are absolute filth  hatecheck   2996   \n",
       "21            82   Immigrants like you are absolute filth.  hatecheck    300   \n",
       "35           103   Immigrants like you are absolute filth.  hatecheck    300   \n",
       "0             31                      wommennnnnn are dirt   dynahate   4838   \n",
       "\n",
       "                                              guideline  votes labelers  \\\n",
       "4369  attack concept associated protected characteri...    1;1      2;4   \n",
       "4469  attack concept associated protected characteri...    1;1      2;4   \n",
       "4476  attack concept associated protected characteri...    1;1      2;4   \n",
       "4492  attack concept associated protected characteri...    1;1      2;4   \n",
       "4475  attack concept associated protected characteri...    1;1      2;4   \n",
       "...                                                 ...    ...      ...   \n",
       "12                                                filth  1;1;1  10;11;2   \n",
       "36                                                filth  1;1;1  10;11;2   \n",
       "21                                                filth  1;1;1  10;11;2   \n",
       "35                                                filth  1;1;1  10;11;2   \n",
       "0                                                 filth  1;1;1  10;11;2   \n",
       "\n",
       "         scores  \n",
       "4369   46.91195  \n",
       "4469  46.975536  \n",
       "4476  47.009098  \n",
       "4492   47.10845  \n",
       "4475  47.176685  \n",
       "...         ...  \n",
       "12    66.559425  \n",
       "36     66.83118  \n",
       "21     67.11209  \n",
       "35     67.11209  \n",
       "0      67.18407  \n",
       "\n",
       "[4651 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b8459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('../postprocess/all_examples_0601_hate.csv', sep = \"\\t\").reset_index(drop=True)\n",
    "\n",
    "grouped = df.groupby('guideline')\n",
    "\n",
    "# 初始化空的DataFrame用于存放结果\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 用于存放临时组\n",
    "temp_group = []\n",
    "\n",
    "# 迭代器，用于遍历每个guideline的行\n",
    "iterators = {name: iter(group.itertuples(index=False)) for name, group in grouped}\n",
    "\n",
    "# 是否还有更多数据需要处理\n",
    "has_more_data = True\n",
    "\n",
    "while has_more_data:\n",
    "    has_more_data = False  # 假设没有更多数据，直到证明确实有\n",
    "    \n",
    "    for name in iterators.keys():\n",
    "        try:\n",
    "            # 尝试从当前guideline的迭代器中获取一行\n",
    "            row = next(iterators[name])\n",
    "            \n",
    "            # 添加这一行到临时组\n",
    "            temp_group.append(row)\n",
    "            \n",
    "            # 确认还有更多数据需要处理\n",
    "            has_more_data = True\n",
    "        except StopIteration:\n",
    "            continue  # 当前guideline的所有行都已被处理，跳过\n",
    "        \n",
    "        # 如果临时组已有5个句子，将其添加到结果DataFrame，然后清空临时组\n",
    "        if len(temp_group) >= 16:\n",
    "            temp_df = pd.DataFrame(temp_group)\n",
    "            result_df = pd.concat([result_df, temp_df])\n",
    "            temp_group = []\n",
    "if len(temp_group) > 0:\n",
    "    temp_df = pd.DataFrame(temp_group)\n",
    "    result_df = pd.concat([result_df, temp_df])\n",
    "result_df.to_csv(\"all_examples_sorted.csv\", sep = \"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9875e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
